# Import unsloth first as requested
import unsloth
from unsloth import FastLanguageModel, is_bfloat16_supported
import torch
import json
from transformers import AutoTokenizer
from opencc import OpenCC
import time

# === Step 1: æ¨¡å‹é…ç½® ===
model_path = "./deepseek-math-7b-orpo-lora-continued"  # èˆ‡training codeä¸­ä¿å­˜è·¯å¾‘ä¸€è‡´
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch_dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16

print(f"Using {device} device with dtype {torch_dtype}")
print(f"Loading model from: {model_path}")

# === Step 2: Prompt æ¨¡æ¿ï¼ˆèˆ‡trainingä¸€è‡´ï¼‰ ===
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. 
Write a response that appropriately completes the request. Please using Traditional Chinese to answer.

### Instruction:
You are an expert physician with extensive knowledge in internal medicine, nephrology, and emergency care.
Analyze the patient case systematically using clinical reasoning:
1. Identify key clinical findings
2. Consider differential diagnoses  
3. Assess severity and urgency
4. Recommend appropriate management

### Patient Case:
{}

### Clinical Analysis:
<think>
{}
</think>

### Medical Assessment:
{}"""

# === Step 3: è¼‰å…¥Fine-tunedæ¨¡å‹ ===
def load_model():
    """è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹"""
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=2048,
            dtype=torch_dtype,
            load_in_4bit=True,
            trust_remote_code=True
        )
        
        # è¨­å®šç‚ºinferenceæ¨¡å¼
        FastLanguageModel.for_inference(model)
        model.to(device)
        
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None, None

# === Step 4: æ¨ç†å‡½æ•¸ ===
def generate_medical_response(model, tokenizer, patient_case, chain_of_thought="", max_new_tokens=400):
    """ç”Ÿæˆé†«å­¸å›ç­”"""
    
    # å‰µå»ºå®Œæ•´prompt
    prompt = alpaca_prompt.format(patient_case, chain_of_thought, "")
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    # ç”Ÿæˆåƒæ•¸ï¼ˆèˆ‡trainingæ™‚æ¸¬è©¦åƒæ•¸ä¿æŒä¸€è‡´ï¼‰
    generation_kwargs = {
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "temperature": 0.3,
        "top_p": 0.9,
        "repetition_penalty": 1.1,
        "eos_token_id": tokenizer.eos_token_id,
        "pad_token_id": tokenizer.eos_token_id,
        "use_cache": True
    }
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        start_time = time.time()
        outputs = model.generate(**inputs, **generation_kwargs)
        end_time = time.time()
    
    # è§£ç¢¼å›ç­”
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    generated_text = response[len(prompt):].strip()
    
    generation_time = end_time - start_time
    
    return {
        "prompt": prompt,
        "response": generated_text,
        "full_response": response,
        "generation_time": generation_time,
        "token_count": len(outputs[0]) - len(inputs["input_ids"][0])
    }

# === Step 5: å…©æ­¥æ¨ç†å‡½æ•¸ï¼ˆèˆ‡åŸå§‹ä»£ç¢¼ç›¸ä¼¼ï¼‰ ===
def two_step_inference(model, tokenizer, patient_case):
    """åŸ·è¡Œå…©æ­¥æ¨ç†éç¨‹"""
    
    print("ğŸ” é–‹å§‹ç¬¬ä¸€æ­¥æ¨ç†...")
    
    # ç¬¬ä¸€æ­¥ï¼šåˆæ­¥åˆ†æ
    first_question = patient_case + " è«‹æ ¹æ“šä¸Šè¿°ç—‡ç‹€é€²è¡Œåˆ†æå’Œæ€è€ƒã€‚"
    first_result = generate_medical_response(model, tokenizer, first_question, "", 300)
    
    print("âœ… ç¬¬ä¸€æ­¥æ¨ç†å®Œæˆ")
    print(f"â±ï¸  ç”Ÿæˆæ™‚é–“: {first_result['generation_time']:.2f}ç§’")
    
    # æå–ç¬¬ä¸€æ­¥çš„ç­”æ¡ˆéƒ¨åˆ†
    first_response = first_result["response"]
    
    # å°‹æ‰¾Answeréƒ¨åˆ†
    answer_start = first_response.find("### Medical Assessment:")
    if answer_start != -1:
        first_analysis = first_response[answer_start + len("### Medical Assessment:"):].strip()
    else:
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šæ¨™è¨˜ï¼Œä½¿ç”¨æ•´å€‹å›æ‡‰
        first_analysis = first_response
    
    print("\nğŸ“‹ ç¬¬ä¸€æ­¥åˆ†æçµæœ:")
    print("=" * 60)
    print(first_analysis[:300] + "..." if len(first_analysis) > 300 else first_analysis)
    print("=" * 60)
    
    print("\nğŸ” é–‹å§‹ç¬¬äºŒæ­¥æ¨ç†...")
    
    # ç¬¬äºŒæ­¥ï¼šåŸºæ–¼ç¬¬ä¸€æ­¥çš„åˆ†æé€²è¡Œæ›´æ·±å…¥çš„æ¨ç†
    second_result = generate_medical_response(model, tokenizer, patient_case, first_analysis, 400)
    
    print("âœ… ç¬¬äºŒæ­¥æ¨ç†å®Œæˆ")
    print(f"â±ï¸  ç”Ÿæˆæ™‚é–“: {second_result['generation_time']:.2f}ç§’")
    
    return {
        "first_step": first_result,
        "second_step": second_result,
        "first_analysis": first_analysis
    }

# === Step 6: ä¸»è¦åŸ·è¡Œå‡½æ•¸ ===
def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    
    print("ğŸš€ é–‹å§‹è¼‰å…¥é†«å­¸AIæ¨¡å‹...")
    print("=" * 70)
    
    # è¼‰å…¥æ¨¡å‹
    model, tokenizer = load_model()
    
    if model is None:
        print("âŒ ç„¡æ³•è¼‰å…¥æ¨¡å‹ï¼Œç¨‹å¼çµæŸ")
        return
    
    # === æ¸¬è©¦æ¡ˆä¾‹ï¼ˆèˆ‡training codeä¸€è‡´ï¼‰ ===
    test_cases = [
        # {
        #     "name": "é€æä½è¡€å£“æ¡ˆä¾‹",
        #     "case": "é†«ç”Ÿæ‚¨å¥½ï¼Œæˆ‘æ˜¯ç—…äººèƒŒæ™¯ 55 æ­²ç”·æ€§ï¼Œçµ‚æœ«æœŸè…ç—…ï¼ˆESRDï¼‰ï¼Œè¡€æ¶²é€æ 3 å¹´ï¼Œæ¯é€± 3 æ¬¡ ä¸»è¨´ æœ€è¿‘å…©é€±é€æä¸­é »ç¹é ­æšˆã€å™å¿ƒï¼Œè¡€å£“æœ€ä½ 80/50 mmHgã€‚è«‹å•æˆ‘å¾—äº†ä»€éº¼ç—…å‘¢ï¼Ÿæ‡‰è©²æ€éº¼è™•ç†ï¼Ÿ"
        # },
        {
            "name": "å¿ƒè¡€ç®¡æ¡ˆä¾‹",
            "case": "65æ­²å¥³æ€§æ‚£è€…ï¼Œç³–å°¿ç—…å²15å¹´ï¼Œæœ€è¿‘å‡ºç¾èƒ¸æ‚¶ã€å‘¼å¸å›°é›£ï¼Œç‰¹åˆ¥æ˜¯æ´»å‹•æ™‚åŠ é‡ã€‚å¿ƒé›»åœ–é¡¯ç¤ºSTæ®µå£“ä½ã€‚è«‹å•å¯èƒ½çš„è¨ºæ–·å’Œè™•ç†å»ºè­°ï¼Ÿ"
        },
        # {
        #     "name": "æ€¥è…¹ç—‡æ¡ˆä¾‹", 
        #     "case": "40æ­²ç”·æ€§ï¼Œå³ä¸Šè…¹ç–¼ç—›2å¤©ï¼Œä¼´ç™¼ç‡’38.5Â°Cï¼Œå™å¿ƒå˜”åã€‚é«”æª¢ç™¼ç¾å³ä¸Šè…¹å£“ç—›ï¼ŒMurphy's signé™½æ€§ã€‚è«‹å•è¨ºæ–·å’Œæ²»ç™‚å»ºè­°ï¼Ÿ"
        # }
    ]

    # test_cases = [
    #     {
    #         "name": "ä¸»å‹•è„ˆå‰é›¢",
    #         "case": "é†«ç”Ÿæ‚¨å¥½ï¼Œä¸€ä½ 58 æ­²ç”·æ€§ï¼Œå·²çŸ¥ é«˜è¡€å£“ 15 å¹´ã€ä¸è¦å‰‡æœè—¥ã€‚æ—©ä¸Šæ–¼å·¥ä½œæ™‚ çªç™¼åŠ‡çƒˆæ’•è£‚æ¨£èƒ¸ç—›ï¼Œè‡ªå‰èƒ¸å‘èƒŒéƒ¨æ¸¸ç§»ï¼Œåˆä½µå†’å†·æ±—èˆ‡é ­æšˆã€‚ åˆ°é™¢æ™‚ BP 190/100 mmHgï¼ˆå³è‡‚ï¼‰ã€160/88 mmHgï¼ˆå·¦è‡‚ï¼‰ï¼ŒHR 110 bpmï¼ŒRR 22/minï¼ŒSpOâ‚‚ 96%ï¼ˆå®¤å…§ç©ºæ°£ï¼‰ã€‚åœ¨ç­‰å¾…å½±åƒç¢ºèªä¹‹éš›ï¼Œä¸‹åˆ—å“ªä¸€é …éœè„ˆçµ¦è—¥æœ€èƒ½æœ‰æ•ˆé™ä½ä¸»å‹•è„ˆå£å‰ªåˆ‡åŠ›ï¼Œæ‡‰ä½œç‚ºåˆå§‹è—¥ç‰©æ²»ç™‚ï¼Ÿ"
    #     },
    #     {
    #         "name": "æ€¥æ€§èƒ°è‡Ÿç‚",
    #         "case": "ä¸€ä½ 48 æ­²ç”·æ€§ï¼Œæœ‰ 10 å¹´é…—é…’å²ï¼ˆæ¯æ—¥ç´„ 250 mL é«˜ç²±é…’ï¼‰ï¼Œæ˜¨æ™šèšé¤å¾Œçªç™¼ æŒçºŒæ€§ä¸Šè…¹éƒ¨åŠ‡ç—›ï¼Œå‘èƒŒéƒ¨æ”¾å°„ï¼Œä¼´å™å¿ƒã€å˜”å 3 æ¬¡ã€‚ åˆ°é™¢æ™‚ BP 100/64 mmHgã€HR 112 bpmã€RR 20/minã€T 37.9 Â°Cï¼Œçš®è†šç¨å†·æ±—ã€‚ æª¢æŸ¥ï¼šä¸Šè…¹å£“ç—›ï¼Œç„¡åå½ˆç—›ï¼›ç„¡é»ƒç–¸ã€‚"
    #     },
    #     {
    #         "name": "ç³–å°¿ç—…é…®é…¸ä¸­æ¯’", 
    #         "case": "ä¸€ä½ 23 æ­²å¥³æ€§ï¼Œå·²çŸ¥ ç¬¬ä¸€å‹ç³–å°¿ç—… 5 å¹´ï¼Œæœ€è¿‘å› èª²æ¥­ç¹å¿™ å…©å¤©æœªæ³¨å°„èƒ°å³¶ç´ ã€‚ ç—‡ç‹€ï¼šå¤šå°¿ã€å£æ¸´ã€å™å¿ƒå˜”å 2 æ¬¡ï¼Œå‘¼å¸æ€¥ä¿ƒã€‚ åˆ°é™¢æ™‚ï¼š BP 90/58 mmHgã€HR 120 bpmã€RR 28 /min (Kussmaul å‘¼å¸)ã€T 37.3 Â°C çš®è†šä¹¾ç‡¥ã€é»è†œä¹¾è£‚"
    #     },
    #     {
    #         "name": "é€æä½è¡€å£“æ¡ˆä¾‹",
    #         "case": "ä¸€ä½ 72 æ­²å¥³æ€§ï¼Œæ—¢å¾€æœ‰ é«˜è¡€å£“ã€å¿ƒæˆ¿é¡«å‹•ï¼ˆæœªè¦å¾‹æœç”¨æŠ—å‡åŠ‘ï¼‰ã€‚ ç—…å²ï¼šä»Šæ™¨ 08:10 èˆ‡å®¶äººè«‡è©±æ™‚çªç™¼ å³å´è‚¢é«”ç„¡åŠ›åˆä½µèªè¨€éšœç¤™ï¼›08:15 å®¶äººå³æ’¥æ‰“ 119ï¼Œ08:55 æŠµé”æ€¥è¨ºã€‚ åˆ°é™¢è©•ä¼°ï¼ˆ09:00ï¼‰ï¼š NIHSS 14 åˆ†ï¼ˆå³å´ä¸Šä¸‹è‚¢ 4/5ã€å…¨çƒå¤±èªã€å‡è¦–åå‘ï¼‰ BP 175/95 mmHgï¼ŒHR 90 bpmï¼ŒRR 18/minï¼ŒT 36.8 Â°C æŒ‡å°–è¡€ç³– 120 mg/dL"
    #     },
    #     {
    #         "name": "æ€¥æ€§ç¼ºè¡€æ€§è…¦ä¸­é¢¨",
    #         "case": "65æ­²å¥³æ€§æ‚£è€…ï¼Œç³–å°¿ç—…å²15å¹´ï¼Œæœ€è¿‘å‡ºç¾èƒ¸æ‚¶ã€å‘¼å¸å›°é›£ï¼Œç‰¹åˆ¥æ˜¯æ´»å‹•æ™‚åŠ é‡ã€‚å¿ƒé›»åœ–é¡¯ç¤ºSTæ®µå£“ä½ã€‚è«‹å•å¯èƒ½çš„è¨ºæ–·å’Œè™•ç†å»ºè­°ï¼Ÿ"
    #     },
    #     {
    #         "name": "å‡ºè¡€æ€§é£Ÿé“éœè„ˆç˜¤", 
    #         "case": "ä¸€ä½ 54 æ­²ç”·æ€§ï¼Œå·²çŸ¥ B å‹è‚ç‚ç›¸é—œè‚ç¡¬åŒ– (Childâ€“Pugh B)ï¼Œè¿‘åŠå¹´æœªå®šæœŸé–€è¨ºè¿½è¹¤ã€‚ ä»Šæ™¨ 06:30 çªç™¼ å¤§é‡é®®ç´…è‰²è¡€å (â‰ˆ300 mL)ï¼Œåˆä½µé»‘ç³ã€‚ 119 é€é†«ï¼Œ07:05 æŠµé”æ€¥è¨ºã€‚ åˆ°é™¢ç”Ÿå‘½å¾µè±¡ï¼ˆ07:10ï¼‰ï¼š BP 88/50 mmHgï¼ŒHR 118 bpmï¼ŒRR 22/minï¼ŒT 36.6 Â°C ç¥æ™ºæ¸…æ¥šï¼Œä½†æ˜é¡¯å£æ¸´ã€çš®è†šæ¿•å†· åˆæ­¥è™•ç½®ï¼šç«‹åˆ»çµ¦äºˆ Oâ‚‚ 3 L/minã€18G é›™å´éœè„ˆè·¯ã€0.9% NaCl 1 L"
    #     },
    #     {
    #         "name": "å¼µåŠ›æ€§æ°£èƒ¸",
    #         "case": "é†«ç”Ÿæ‚¨å¥½ï¼Œæ€¥è¨ºæ”¶æ²»ä¸€ä½ 28 æ­²ç”·æ€§ï¼Œç„¡æ˜é¡¯ç—…å²ï¼Œé¨æ©Ÿè»Šèˆ‡æ±½è»Šå´æ’å¾Œè¢«é€è‡³é™¢ã€‚ åˆ°é™¢æ™‚ï¼ˆT0ï¼‰ï¼š BP 80/50 mmHgã€HR 132 bpmã€RR 34 / minã€SpOâ‚‚ 86 %ï¼ˆå®¤å…§ç©ºæ°£ï¼‰ æ„è­˜ï¼šç…©èºä¸å®‰ï¼Œæ–·çºŒå–Šã€Œå–˜ä¸éæ°£ã€ çš®è†šï¼šæ¿•å†·ã€è’¼ç™½ æŸ¥é«”ï¼ˆT + 2 minï¼‰ï¼š é ­é ¸ï¼šé ¸éœè„ˆæ€’å¼µ èƒ¸éƒ¨ï¼šå³èƒ¸ç„¡æ˜é¡¯é–‹æ”¾æ€§å‚·å£ï¼›å³å´å‘¼å¸éŸ³å¹¾ä¹æ¶ˆå¤±ï¼Œå©è¨ºé¼“éŸ³ï¼›æ°£ç®¡è¼•åº¦å·¦å å››è‚¢ï¼šæ¯›ç´°è¡€ç®¡å†å……ç›ˆå»¶é²ï¼ˆ>3 ç§’ï¼‰ ç—…æ­·è¨Šæ¯ï¼š æ•‘è­·å“¡å ±å‘Šç¾å ´æœªè¦‹å¤§é‡å‡ºè¡€ï¼›é€”ä¸­å·²è¼¸ 500 mL ç”Ÿç†é£Ÿé¹½æ°´ä½†è¡€å£“ç„¡æ˜é¡¯å›å‡ã€‚æ­¤æ™‚æœ€æ‡‰ç«‹å³åŸ·è¡Œä¸‹åˆ—å“ªä¸€é …è™•ç½®ï¼Ÿ"
    #     },
    #     {
    #         "name": "æ•—è¡€æ€§ä¼‘å…‹",
    #         "case": "ä¸€ä½ 67 æ­²ç”·æ€§ï¼Œæ…¢æ€§é˜»å¡æ€§è‚ºç—…ï¼ˆCOPDï¼‰ç—…å²ï¼Œè¿‘ä¸‰æ—¥å’³å—½ã€é»ƒç—°ä¼´ç™¼ç‡’ã€‚ä»Šæ—¥ä¸Šåˆ 08:20 å› æ„è­˜æ··äº‚è¢«å®¶å±¬é€è‡³æ€¥è¨ºã€‚ åˆ°é™¢ï¼ˆ08:40ï¼‰ç”Ÿå‘½å¾µè±¡ BP 78/40 mmHgï¼ˆMean â‰ˆ 53 mmHgï¼‰ HR 122 bpm RR 28 /min T 39.2 Â°C SpOâ‚‚ 92 %ï¼ˆé¢ç½© 6 L/minï¼‰ï¼Œå·²å®Œæˆåˆæ­¥è¼¸æ¶²ï¼ˆç¸½é‡ 30 mL/kgï¼‰å¾Œä»è¡€å£“åä½ï¼Œä»¥ä¸‹ä½•è€…ç‚ºç¬¬ä¸€ç·šè¡€ç®¡å‡å£“åŠ‘ä»¥ç¶­æŒç›®æ¨™å¹³å‡å‹•è„ˆå£“ï¼ˆMAP â‰¥ 65 mmHgï¼‰ï¼Ÿ"
    #     },
    #     {
    #         "name": "é€æä½è¡€å£“æ¡ˆä¾‹",
    #         "case": "é†«ç”Ÿæ‚¨å¥½ï¼Œæˆ‘æ˜¯ç—…äººèƒŒæ™¯ 55 æ­²ç”·æ€§ï¼Œçµ‚æœ«æœŸè…ç—…ï¼ˆESRDï¼‰ï¼Œè¡€æ¶²é€æ 3 å¹´ï¼Œæ¯é€± 3 æ¬¡ ä¸»è¨´ æœ€è¿‘å…©é€±é€æä¸­é »ç¹é ­æšˆã€å™å¿ƒï¼Œè¡€å£“æœ€ä½ 80/50 mmHgã€‚è«‹å•æˆ‘å¾—äº†ä»€éº¼ç—…å‘¢ï¼Ÿæ‡‰è©²æ€éº¼è™•ç†ï¼Ÿ"
    #     },
    #     {
    #         "name": "å¿ƒè¡€ç®¡æ¡ˆä¾‹",
    #         "case": "65æ­²å¥³æ€§æ‚£è€…ï¼Œç³–å°¿ç—…å²15å¹´ï¼Œæœ€è¿‘å‡ºç¾èƒ¸æ‚¶ã€å‘¼å¸å›°é›£ï¼Œç‰¹åˆ¥æ˜¯æ´»å‹•æ™‚åŠ é‡ã€‚å¿ƒé›»åœ–é¡¯ç¤ºSTæ®µå£“ä½ã€‚è«‹å•å¯èƒ½çš„è¨ºæ–·å’Œè™•ç†å»ºè­°ï¼Ÿ"
    #     },
    # ]
    
    # === åŸ·è¡Œæ¨ç†æ¸¬è©¦ ===
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'ğŸ¥ ' + '=' * 20} æ¸¬è©¦æ¡ˆä¾‹ {i}: {test_case['name']} {'=' * 20}")
        print(f"ğŸ“ ç—…ä¾‹æè¿°: {test_case['case']}")
        print("\n" + "=" * 70)
        
        try:
            # åŸ·è¡Œå…©æ­¥æ¨ç†
            results = two_step_inference(model, tokenizer, test_case['case'])
            
            # é¡¯ç¤ºæœ€çµ‚çµæœ
            final_response = results["second_step"]["response"]
            
            print(f"\nğŸ¯ æœ€çµ‚é†«å­¸å»ºè­°:")
            print("=" * 70)
            print(final_response)
            print("=" * 70)
            
            print(f"\nğŸ“Š çµ±è¨ˆè³‡è¨Š:")
            print(f"   - ç¬¬ä¸€æ­¥ç”Ÿæˆtokenæ•¸: {results['first_step']['token_count']}")
            print(f"   - ç¬¬äºŒæ­¥ç”Ÿæˆtokenæ•¸: {results['second_step']['token_count']}")
            print(f"   - ç¸½è™•ç†æ™‚é–“: {results['first_step']['generation_time'] + results['second_step']['generation_time']:.2f}ç§’")
            
            # å¦‚æœéœ€è¦ç¿»è­¯ï¼ˆæª¢æŸ¥æ˜¯å¦æœ‰è‹±æ–‡å…§å®¹ï¼‰
            if any(ord(char) < 128 and char.isalpha() for char in final_response):
                print(f"\nğŸŒ åµæ¸¬åˆ°è‹±æ–‡å…§å®¹ï¼Œæ˜¯å¦éœ€è¦ç¿»è­¯ï¼Ÿï¼ˆé€™è£¡å¯ä»¥æ·»åŠ ç¿»è­¯åŠŸèƒ½ï¼‰")
            
            print("\n" + "ğŸ”„ " + "=" * 68 + "\n")
            
        except Exception as e:
            print(f"âŒ è™•ç†æ¡ˆä¾‹ {i} æ™‚å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
    
    print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")

# === Step 7: äº’å‹•å¼æ¨ç†å‡½æ•¸ ===
def interactive_inference():
    """äº’å‹•å¼æ¨ç†æ¨¡å¼"""
    
    print("ğŸš€ è¼‰å…¥æ¨¡å‹...")
    model, tokenizer = load_model()
    
    if model is None:
        return
    
    print("\nğŸ’¬ é€²å…¥äº’å‹•æ¨¡å¼ï¼ˆè¼¸å…¥ 'quit' çµæŸï¼‰")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ‘¨â€âš•ï¸ è«‹æè¿°ç—…ä¾‹: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ å†è¦‹ï¼")
                break
            
            if not user_input:
                print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆçš„ç—…ä¾‹æè¿°")
                continue
            
            print(f"\nğŸ” æ­£åœ¨åˆ†æç—…ä¾‹...")
            
            # åŸ·è¡Œæ¨ç†
            results = two_step_inference(model, tokenizer, user_input)
            
            print(f"\nğŸ¯ é†«å­¸å»ºè­°:")
            print("-" * 50)
            print(results["second_step"]["response"])
            print("-" * 50)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹å¼ä¸­æ–·ï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"âŒ è™•ç†æ™‚å‡ºéŒ¯: {e}")

# === Step 8: å‘½ä»¤åˆ—é¸é … ===
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        interactive_inference()
    else:
        main()
