from flask import Flask, render_template, request, jsonify, stream_template
import unsloth
from unsloth import FastLanguageModel, is_bfloat16_supported
import torch
import json
from transformers import AutoTokenizer
import time
import threading

app = Flask(__name__)

# å…¨å±€è®Šé‡å­˜å„²æ¨¡å‹
model = None
tokenizer = None
model_loaded = False

# === æ¨¡å‹é…ç½® ===
model_path = "./deepseek-math-7b-orpo-lora-continued"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch_dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16

# === Prompt æ¨¡æ¿ ===
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. 
Write a response that appropriately completes the request. Please using Traditional Chinese to answer.

### Instruction:
You are an expert physician with extensive knowledge in internal medicine, nephrology, and emergency care.
Analyze the patient case systematically using clinical reasoning:
1. Identify key clinical findings
2. Consider differential diagnoses  
3. Assess severity and urgency
4. Recommend appropriate management

### Patient Case:
{}

### Clinical Analysis:
<think>
{}
</think>

### Medical Assessment:
{}"""

def load_model():
    """è¼‰å…¥æ¨¡å‹"""
    global model, tokenizer, model_loaded
    try:
        print("ğŸš€ é–‹å§‹è¼‰å…¥æ¨¡å‹...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=2048,
            dtype=torch_dtype,
            load_in_4bit=True,
            trust_remote_code=True
        )
        
        FastLanguageModel.for_inference(model)
        model.to(device)
        model_loaded = True
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        model_loaded = False

def generate_medical_response(patient_case, chain_of_thought="", max_new_tokens=400):
    """ç”Ÿæˆé†«å­¸å›ç­”"""
    if not model_loaded:
        return {"error": "æ¨¡å‹å°šæœªè¼‰å…¥å®Œæˆ"}
    
    prompt = alpaca_prompt.format(patient_case, chain_of_thought, "")
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    generation_kwargs = {
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "temperature": 0.3,
        "top_p": 0.9,
        "repetition_penalty": 1.1,
        "eos_token_id": tokenizer.eos_token_id,
        "pad_token_id": tokenizer.eos_token_id,
        "use_cache": True
    }
    
    with torch.no_grad():
        start_time = time.time()
        outputs = model.generate(**inputs, **generation_kwargs)
        end_time = time.time()
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated_text = response[len(prompt):].strip()
    
    return {
        "response": generated_text,
        "generation_time": end_time - start_time,
        "token_count": len(outputs[0]) - len(inputs["input_ids"][0])
    }

def two_step_inference(patient_case):
    """åŸ·è¡Œå…©æ­¥æ¨ç†"""
    if not model_loaded:
        return {"error": "æ¨¡å‹å°šæœªè¼‰å…¥å®Œæˆ"}
    
    # ç¬¬ä¸€æ­¥æ¨ç†
    first_question = patient_case + " è«‹æ ¹æ“šä¸Šè¿°ç—‡ç‹€é€²è¡Œåˆ†æå’Œæ€è€ƒã€‚"
    first_result = generate_medical_response(first_question, "", 300)
    
    if "error" in first_result:
        return first_result
    
    first_response = first_result["response"]
    answer_start = first_response.find("### Medical Assessment:")
    if answer_start != -1:
        first_analysis = first_response[answer_start + len("### Medical Assessment:"):].strip()
    else:
        first_analysis = first_response
    
    # ç¬¬äºŒæ­¥æ¨ç†
    second_result = generate_medical_response(patient_case, first_analysis, 400)
    
    if "error" in second_result:
        return second_result
    
    return {
        "first_step": first_result,
        "second_step": second_result,
        "first_analysis": first_analysis,
        "final_response": second_result["response"],
        "total_time": first_result["generation_time"] + second_result["generation_time"]
    }

@app.route('/')
def index():
    return render_template('index.html', model_loaded=model_loaded)

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        patient_case = data.get('patient_case', '').strip()
        
        if not patient_case:
            return jsonify({"error": "è«‹è¼¸å…¥ç—…ä¾‹æè¿°"})
        
        if not model_loaded:
            return jsonify({"error": "æ¨¡å‹å°šæœªè¼‰å…¥å®Œæˆï¼Œè«‹ç¨å¾Œå†è©¦"})
        
        # åŸ·è¡Œæ¨ç†
        result = two_step_inference(patient_case)
        
        if "error" in result:
            return jsonify(result)
        
        return jsonify({
            "success": True,
            "final_response": result["final_response"],
            "generation_time": f"{result['total_time']:.2f}",
            "first_step_tokens": result["first_step"]["token_count"],
            "second_step_tokens": result["second_step"]["token_count"]
        })
        
    except Exception as e:
        return jsonify({"error": f"è™•ç†è«‹æ±‚æ™‚å‡ºéŒ¯: {str(e)}"})

@app.route('/status')
def status():
    return jsonify({"model_loaded": model_loaded})

# if __name__ == '__main__':
#     # åœ¨èƒŒæ™¯ç·šç¨‹ä¸­è¼‰å…¥æ¨¡å‹
#     threading.Thread(target=load_model, daemon=True).start()
    
#     # å•Ÿå‹• Flask æ‡‰ç”¨ï¼Œç¶å®šåˆ°æ‰€æœ‰ç¶²è·¯ä»‹é¢
#     app.run(host='0.0.0.0', port=5000, debug=False)

if __name__ == '__main__':
    # åœ¨èƒŒæ™¯ç·šç¨‹ä¸­è¼‰å…¥æ¨¡å‹
    threading.Thread(target=load_model, daemon=True).start()
    
    # ä½¿ç”¨å·²æ˜ å°„çš„ç«¯å£ 8080 æˆ– 8888
    app.run(host='0.0.0.0', port=8080, debug=False)  # ä½¿ç”¨ 8080
    # æˆ–è€…
    # app.run(host='0.0.0.0', port=8888, debug=False)  # ä½¿ç”¨ 8888

